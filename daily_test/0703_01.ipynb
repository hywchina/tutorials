{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequence = torch.tensor([[0.1, 0.2, 0.3, 0.5], [0.4, 0.5, 0.6, 0.7], [0.7, 0.8, 0.9, 1.0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_weights_key = torch.randn(input_sequence.size(-1), input_sequence.size(-1))\n",
    "random_weights_query = torch.randn(input_sequence.size(-1), input_sequence.size(-1))\n",
    "random_weights_value = torch.randn(input_sequence.size(-1), input_sequence.size(-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算 Key、Query 和 Value 矩阵\n",
    "key = torch.matmul(input_sequence, random_weights_key)\n",
    "query = torch.matmul(input_sequence, random_weights_query)\n",
    "value = torch.matmul(input_sequence, random_weights_value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6722, -0.8577, -1.1768],\n",
       "        [-1.0108, -1.2007, -1.6163],\n",
       "        [-1.4273, -1.6372, -2.1814]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_scores = torch.matmul(query, key.T) / torch.sqrt(torch.tensor(query.size(-1), dtype=torch.float32))\n",
    "attention_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4108, 0.3412, 0.2480],\n",
       "        [0.4214, 0.3485, 0.2300],\n",
       "        [0.4384, 0.3554, 0.2062]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6722, -0.8577, -1.1768],\n",
       "        [-1.0108, -1.2007, -1.6163],\n",
       "        [-1.4273, -1.6372, -2.1814]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(query, key.T) / torch.sqrt(torch.tensor(query.size(-1), dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5961, -1.5240,  0.7995, -0.3255],\n",
       "        [-1.5702, -1.4992,  0.7820, -0.3243],\n",
       "        [-1.5336, -1.4642,  0.7570, -0.3227]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = torch.matmul(attention_weights, value)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.0923)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "scalar = torch.randn(())\n",
    "print(scalar)  # 一个服从标准正态分布的随机数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([1., 2., 3.], requires_grad=True)\n",
      "y: tensor([3., 4., 5.], grad_fn=<AddBackward0>)\n",
      "z: tensor([18., 32., 50.], grad_fn=<MulBackward0>)\n",
      "x.grad: tensor([ 216.,  512., 1000.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 创建一个张量并启用 requires_grad\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "\n",
    "# 进行一些操作\n",
    "y = x + 2\n",
    "z = y * y * 2\n",
    "out = z.mean()\n",
    "\n",
    "print(\"x:\", x)\n",
    "print(\"y:\", y)\n",
    "print(\"z:\", z)\n",
    "# print(\"out:\", out)\n",
    "\n",
    "# 反向传播计算梯度\n",
    "z.backward(gradient=z)\n",
    "\n",
    "\n",
    "# x 的梯度 (存储在 x.grad 中)\n",
    "print(\"x.grad:\", x.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.2398, 0.4022, 0.5029, 0.1586, 0.6830],\n",
       "          [0.2595, 0.7700, 0.0521, 0.2001, 0.1069],\n",
       "          [0.4462, 0.6837, 0.6259, 0.7133, 0.4479],\n",
       "          [0.9324, 0.8798, 0.6851, 0.4031, 0.9305]],\n",
       "\n",
       "         [[0.0546, 0.0280, 0.2939, 0.1642, 0.8514],\n",
       "          [0.3091, 0.3258, 0.1811, 0.9448, 0.4614],\n",
       "          [0.6802, 0.3815, 0.2783, 0.3688, 0.0626],\n",
       "          [0.4162, 0.9296, 0.5085, 0.9594, 0.5284]],\n",
       "\n",
       "         [[0.8062, 0.9262, 0.0049, 0.8470, 0.8719],\n",
       "          [0.1529, 0.3640, 0.5178, 0.1427, 0.6948],\n",
       "          [0.9093, 0.9645, 0.9951, 0.0249, 0.3401],\n",
       "          [0.5244, 0.4705, 0.8384, 0.6986, 0.3982]]],\n",
       "\n",
       "\n",
       "        [[[0.3143, 0.2618, 0.3191, 0.8092, 0.8274],\n",
       "          [0.1319, 0.4903, 0.8877, 0.0393, 0.4391],\n",
       "          [0.2902, 0.6614, 0.3019, 0.0026, 0.3177],\n",
       "          [0.4606, 0.7559, 0.7139, 0.1065, 0.6123]],\n",
       "\n",
       "         [[0.3965, 0.7048, 0.6530, 0.6256, 0.8849],\n",
       "          [0.8202, 0.9877, 0.7630, 0.2774, 0.4079],\n",
       "          [0.0238, 0.1944, 0.6778, 0.4352, 0.4888],\n",
       "          [0.4939, 0.4604, 0.5104, 0.5403, 0.7242]],\n",
       "\n",
       "         [[0.1328, 0.8126, 0.6184, 0.8190, 0.1051],\n",
       "          [0.6092, 0.5536, 0.9399, 0.4349, 0.6040],\n",
       "          [0.1801, 0.5754, 0.6392, 0.4155, 0.6326],\n",
       "          [0.8958, 0.8849, 0.5366, 0.6977, 0.0043]]]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(2, 3, 4, 5)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.1679, 0.1975, 0.2184, 0.1548, 0.2615],\n",
       "          [0.1894, 0.3156, 0.1539, 0.1785, 0.1626],\n",
       "          [0.1732, 0.2197, 0.2073, 0.2263, 0.1735],\n",
       "          [0.2317, 0.2198, 0.1809, 0.1364, 0.2312]],\n",
       "\n",
       "         [[0.1520, 0.1480, 0.1931, 0.1696, 0.3372],\n",
       "          [0.1681, 0.1709, 0.1479, 0.3174, 0.1957],\n",
       "          [0.2716, 0.2014, 0.1817, 0.1989, 0.1464],\n",
       "          [0.1513, 0.2529, 0.1660, 0.2605, 0.1693]],\n",
       "\n",
       "         [[0.2134, 0.2406, 0.0958, 0.2223, 0.2279],\n",
       "          [0.1566, 0.1934, 0.2256, 0.1550, 0.2693],\n",
       "          [0.2424, 0.2562, 0.2641, 0.1001, 0.1372],\n",
       "          [0.1856, 0.1759, 0.2541, 0.2209, 0.1636]]],\n",
       "\n",
       "\n",
       "        [[[0.1596, 0.1515, 0.1604, 0.2619, 0.2667],\n",
       "          [0.1464, 0.2095, 0.3117, 0.1334, 0.1990],\n",
       "          [0.1909, 0.2767, 0.1931, 0.1432, 0.1962],\n",
       "          [0.1819, 0.2444, 0.2343, 0.1277, 0.2117]],\n",
       "\n",
       "         [[0.1529, 0.2081, 0.1976, 0.1923, 0.2492],\n",
       "          [0.2288, 0.2706, 0.2161, 0.1330, 0.1515],\n",
       "          [0.1387, 0.1645, 0.2667, 0.2093, 0.2208],\n",
       "          [0.1890, 0.1828, 0.1922, 0.1980, 0.2380]],\n",
       "\n",
       "         [[0.1323, 0.2611, 0.2150, 0.2628, 0.1287],\n",
       "          [0.1933, 0.1829, 0.2691, 0.1624, 0.1923],\n",
       "          [0.1448, 0.2150, 0.2292, 0.1833, 0.2277],\n",
       "          [0.2552, 0.2525, 0.1782, 0.2094, 0.1047]]]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = F.softmax(x, dim=-1)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000, 1.0000, 1.0000, 1.0000],\n",
       "         [1.0000, 1.0000, 1.0000, 1.0000],\n",
       "         [1.0000, 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "        [[1.0000, 1.0000, 1.0000, 1.0000],\n",
       "         [1.0000, 1.0000, 1.0000, 1.0000],\n",
       "         [1.0000, 1.0000, 1.0000, 1.0000]]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = y.sum(dim=-1)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.optim as optim \n",
    "import torch.nn.functional as F \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "        18, 19])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(0,20)\n",
    "b = torch.randn(20)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.FloatTensor'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0252, -1.3866, -0.2124,  0.1827,  0.0384, -1.7722, -0.1030,  0.2198,\n",
       "         0.2870, -0.2933,  0.5405, -0.8020,  2.1565, -0.0025,  0.3317, -0.4555,\n",
       "        -0.9157,  1.1668, -1.0764,  0.4816])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8150, -0.2135, -0.2431],\n",
       "        [ 0.0481, -0.4257, -1.2807]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(2,3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8150,  0.0481],\n",
       "        [-0.2135, -0.4257],\n",
       "        [-0.2431, -1.2807]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.transpose(-1,-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 5 \n",
    "output_size = 2 \n",
    "batch_size = 30\n",
    "data_size = 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomDataset(Dataset):\n",
    "    def __init__(self, size, length):\n",
    "        self.len = length\n",
    "        self.data = torch.randn(length, size)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_loader = DataLoader(dataset=RandomDataset(input_size, data_size),batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, output_size) -> None:\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(input_size, output_size)\n",
    "    def forward(self, input):\n",
    "        output = self.fc(input)\n",
    "        print(input.size(), output.size())\n",
    "\n",
    "        return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (fc): Linear(in_features=5, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(input_size, output_size)\n",
    "if torch.cuda.device_count() > 1:\n",
    "  print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "  # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n",
    "  model = nn.DataParallel(model)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 5]) torch.Size([30, 2])\n",
      "Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n",
      "torch.Size([30, 5]) torch.Size([30, 2])\n",
      "Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n",
      "torch.Size([30, 5]) torch.Size([30, 2])\n",
      "Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n",
      "torch.Size([30, 5]) torch.Size([30, 2])\n",
      "Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n",
      "torch.Size([30, 5]) torch.Size([30, 2])\n",
      "Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n",
      "torch.Size([30, 5]) torch.Size([30, 2])\n",
      "Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n",
      "torch.Size([20, 5]) torch.Size([20, 2])\n",
      "Outside: input size torch.Size([20, 5]) output_size torch.Size([20, 2])\n"
     ]
    }
   ],
   "source": [
    "for data in rand_loader:\n",
    "    input = data.to(device)\n",
    "    output = model(input)\n",
    "    print(\"Outside: input size\", input.size(),\n",
    "          \"output_size\", output.size())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
