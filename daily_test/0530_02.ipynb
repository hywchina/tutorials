{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,PIL \n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch \n",
    "from torch import nn \n",
    "\n",
    "import torchvision \n",
    "from torchvision import transforms\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#======================================================================\n",
    "# import accelerate\n",
    "from accelerate import Accelerator\n",
    "from accelerate.utils import set_seed\n",
    "#======================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(batch_size=64):\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "    ds_train = torchvision.datasets.MNIST(root=\"./mnist/\",train=True,download=True,transform=transform)\n",
    "    ds_val = torchvision.datasets.MNIST(root=\"./mnist/\",train=False,download=True,transform=transform)\n",
    "\n",
    "    dl_train =  torch.utils.data.DataLoader(ds_train, batch_size=batch_size, shuffle=True,\n",
    "                                            num_workers=2,drop_last=True)\n",
    "    dl_val =  torch.utils.data.DataLoader(ds_val, batch_size=batch_size, shuffle=False, \n",
    "                                          num_workers=2,drop_last=True)\n",
    "    return dl_train,dl_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_net():\n",
    "    net = nn.Sequential()\n",
    "    net.add_module(\"conv1\",nn.Conv2d(in_channels=1,out_channels=512,kernel_size = 3))\n",
    "    net.add_module(\"pool1\",nn.MaxPool2d(kernel_size = 2,stride = 2)) \n",
    "    net.add_module(\"conv2\",nn.Conv2d(in_channels=512,out_channels=256,kernel_size = 5))\n",
    "    net.add_module(\"pool2\",nn.MaxPool2d(kernel_size = 2,stride = 2))\n",
    "    net.add_module(\"dropout\",nn.Dropout2d(p = 0.1))\n",
    "    net.add_module(\"adaptive_pool\",nn.AdaptiveMaxPool2d((1,1)))\n",
    "    net.add_module(\"flatten\",nn.Flatten())\n",
    "    net.add_module(\"linear1\",nn.Linear(256,128))\n",
    "    net.add_module(\"relu\",nn.ReLU())\n",
    "    net.add_module(\"linear2\",nn.Linear(128,10))\n",
    "    return net "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(epochs = 5,\n",
    "                  lr = 1e-3,\n",
    "                  batch_size= 1024,\n",
    "                  ckpt_path = \"checkpoint.pt\",\n",
    "                  mixed_precision=\"no\", #'fp16' or 'bf16'\n",
    "                 ):\n",
    "    \n",
    "    train_dataloader, eval_dataloader = create_dataloaders(batch_size)\n",
    "    model = create_net()\n",
    "    \n",
    "\n",
    "    optimizer = torch.optim.AdamW(params=model.parameters(), lr=lr)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer=optimizer, max_lr=25*lr, \n",
    "                              epochs=epochs, steps_per_epoch=len(train_dataloader))\n",
    "    \n",
    "    #======================================================================\n",
    "    # initialize accelerator and auto move data/model to accelerator.device\n",
    "    set_seed(42)\n",
    "    accelerator = Accelerator(mixed_precision=mixed_precision)\n",
    "    accelerator.print(f'device {str(accelerator.device)} is used!')\n",
    "    model, optimizer,lr_scheduler, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "        model, optimizer,lr_scheduler, train_dataloader, eval_dataloader)\n",
    "    #======================================================================\n",
    "    \n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            features,labels = batch\n",
    "            preds = model(features)\n",
    "            loss = nn.CrossEntropyLoss()(preds,labels)\n",
    "            \n",
    "            #======================================================================\n",
    "            #attention here! \n",
    "            accelerator.backward(loss) #loss.backward()\n",
    "            #======================================================================\n",
    "            \n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "        \n",
    "        model.eval()\n",
    "        accurate = 0\n",
    "        num_elems = 0\n",
    "        \n",
    "        for _, batch in enumerate(eval_dataloader):\n",
    "            features,labels = batch\n",
    "            with torch.no_grad():\n",
    "                preds = model(features)\n",
    "            predictions = preds.argmax(dim=-1)\n",
    "            \n",
    "            #======================================================================\n",
    "            #gather data from multi-gpus (used when in ddp mode)\n",
    "            predictions = accelerator.gather_for_metrics(predictions)\n",
    "            labels = accelerator.gather_for_metrics(labels)\n",
    "            #======================================================================\n",
    "            \n",
    "            accurate_preds =  (predictions==labels)\n",
    "            num_elems += accurate_preds.shape[0]\n",
    "            accurate += accurate_preds.long().sum()\n",
    "\n",
    "        eval_metric = accurate.item() / num_elems\n",
    "        \n",
    "        #======================================================================\n",
    "        #print logs and save ckpt  \n",
    "        accelerator.wait_for_everyone()\n",
    "        nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        accelerator.print(f\"epoch【{epoch}】@{nowtime} --> eval_metric= {100 * eval_metric:.2f}%\")\n",
    "        net_dict = accelerator.get_state_dict(model)\n",
    "        accelerator.save(net_dict,ckpt_path+\"_\"+str(epoch))\n",
    "        #======================================================================\n",
    "        \n",
    "#training_loop(epochs = 5,lr = 1e-3,batch_size= 1024,ckpt_path = \"checkpoint.pt\",\n",
    "#             mixed_precision=\"no\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./mnist/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:20<00:00, 491183.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist/MNIST/raw/train-images-idx3-ubyte.gz to ./mnist/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./mnist/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 133871.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist/MNIST/raw/train-labels-idx1-ubyte.gz to ./mnist/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./mnist/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:01<00:00, 1282770.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist/MNIST/raw/t10k-images-idx3-ubyte.gz to ./mnist/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 1645549.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./mnist/MNIST/raw\n",
      "\n",
      "device cuda is used!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch【0】@2024-05-30 07:31:40 --> eval_metric= 96.88%\n",
      "[2024-05-30 07:31:40,785] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huyanwei/anaconda3/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible\n",
      "epoch【1】@2024-05-30 07:31:47 --> eval_metric= 96.35%\n",
      "epoch【2】@2024-05-30 07:31:53 --> eval_metric= 98.10%\n",
      "epoch【3】@2024-05-30 07:31:59 --> eval_metric= 98.23%\n",
      "epoch【4】@2024-05-30 07:32:05 --> eval_metric= 98.45%\n"
     ]
    }
   ],
   "source": [
    "training_loop(epochs = 5,lr = 1e-4,batch_size= 1024,\n",
    "              ckpt_path = \"checkpoint.pt\",\n",
    "              mixed_precision=\"no\") #mixed_precision='fp16' or  'bf16'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (conv1): Conv2d(1, 512, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(512, 256, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (dropout): Dropout2d(p=0.1, inplace=False)\n",
      "  (adaptive_pool): AdaptiveMaxPool2d(output_size=(1, 1))\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear1): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (linear2): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = create_net()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving './demo.pth' at http://localhost:8080\n",
      "Serving './demo.pth' at http://localhost:8081\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('localhost', 8081)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 针对有网络模型，但还没有训练保存 .pth 文件的情况\n",
    "import netron\n",
    "import torch.onnx\n",
    "from torch.autograd import Variable\n",
    "from torchvision.models import resnet18  # 以 resnet18 为例\n",
    "\n",
    "myNet = resnet18()  # 实例化 resnet18\n",
    "x = torch.randn(16, 3, 40, 40)  # 随机生成一个输入\n",
    "modelData = \"./demo.pth\"  # 定义模型数据保存的路径\n",
    "# modelData = \"./demo.onnx\"  # 有人说应该是 onnx 文件，但我尝试 pth 是可以的 \n",
    "torch.onnx.export(myNet, x, modelData)  # 将 pytorch 模型以 onnx 格式导出并保存\n",
    "netron.start(modelData)  # 输出网络结构\n",
    "\n",
    "#  针对已经存在网络模型 .pth 文件的情况\n",
    "import netron\n",
    "\n",
    "modelData = \"./demo.pth\"  # 定义模型数据保存的路径\n",
    "netron.start(modelData)  # 输出网络结构\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
